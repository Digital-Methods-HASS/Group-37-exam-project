---
title: "Reynolds"
author: "Julie Overgaard"
date: "2024-05-01"
output: html_document
---

## Setting up the right packages:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)
library(tidyverse)
library(here)

# For text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

dir.create("dataRey")

```

## Getting the Reynolds pdf text:

```{r}
reynolds <- pdf_text('dataRey/Reynolds, The Use of Feudalism in Comparative History.pdf')
reynolds_df <- data.frame(text = reynolds) %>% 
  mutate(page = 1:n())

reynolds_text <- reynolds_df %>% 
  filter(page %in% 1:20) %>% 
  mutate(text = str_split(text, '\n')) %>% 
  unnest(text)
```

## Cut out the first pages
-Setting it up so reynolds_true contains only Reynolds text
```{r}
reynolds_true <- reynolds[3:30]
```


### Some wrangling:
- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r}
reynolds_df <- data.frame(reynolds_true) %>% 
  mutate(text_full1 = str_split(reynolds_true, pattern = '\n')) %>% 
  unnest(text_full1) %>% 
  mutate(text_full1 = str_trim(text_full1))
view(reynolds_df)
```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r}
reynolds_tokens <- reynolds_df %>% 
  unnest_tokens(word, text_full1)
```

## Counting the words
```{r}
reynolds_wc <- reynolds_tokens %>% 
  count(word) %>% 
  arrange(-n)
reynolds_wc
```

## Removing stop words

```{r}
reynolds_stop <- reynolds_tokens %>% 
  anti_join(stop_words) %>% 
  select(-reynolds_true)
```

## Counting with the stop words

```{r}
reynolds_swc <- reynolds_stop %>% 
  count(word) %>% 
  arrange(-n)
reynolds_swc
```

## Creating a word cloud

```{r}
length(unique(reynolds_stop$word))
## 1624 unique words

## Getting the top 100 words
reynolds_top100 <- reynolds_stop %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
reynolds_top100
```


```{r}
reynolds_cloud <- ggplot(data = reynolds_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()
reynolds_cloud
```

## Customizing the cloud

```{r}
ggplot(data = reynolds_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

## Sentiment Analysis - aflinn
-The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.
-"afinn": Words ranked from -5 (very negative) to +5 (very positive)

First, bind words in `mckay_stop` to `afinn` lexicon:
```{r}
reynolds_afinn <- reynolds_stop %>% 
  inner_join(get_sentiments("afinn"))
reynolds_afinn
```

Let's find some counts (by sentiment ranking):
```{r}
reynolds_afinn_hist <- reynolds_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = reynolds_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```

Looking closer on some of the positive words (1)
```{r}
reynolds_afinn1 <- reynolds_afinn %>% 
  filter(value == 1) %>% 
  head(50)
reynolds_afinn1
```



```{r}
# Check the unique 1-score words:
unique(reynolds_afinn1$word)

# Count & plot them
reynolds_afinn1_n <- reynolds_afinn1 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = reynolds_afinn1_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()
```


Summarize sentiment
-Mean =
-Median =
```{r}
reynolds_summary <- reynolds_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
reynolds_summary
```

-The mean and median indicate *slightly* positive overall sentiments based on the AFINN lexicon. 

## Sentiment analysis - nrc
-The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.
-We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the idig non-stopword text with the nrc lexicon: 

```{r}
reynolds_nrc <- reynolds_stop %>% 
  inner_join(get_sentiments("nrc"))
reynolds_nrc
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r}
reynolds_exclude <- reynolds_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(reynolds_exclude)

# Count to find the most excluded:
reynolds_exclude_n <- reynolds_exclude %>% 
  count(word, sort = TRUE)

head(reynolds_exclude_n)
```


Counts
```{r}
reynolds_nrc_n <- reynolds_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = reynolds_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()
```

Or count by sentiment *and* word, then facet:
```{r}
reynolds_nrc_n5 <- reynolds_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

reynolds_nrc_gg <- ggplot(data = reynolds_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
reynolds_nrc_gg
```

## Big picture takeaway

There are serious limitations of sentiment analysis using existing lexicons, and you should **think really hard** about your findings and if a lexicon makes sense for your study. Otherwise, word counts and exploration alone can be useful! 

### Credits: 
This tutorial is inspired by Allison Horst's Advanced Statistics and Data Analysis.